# DPO Configuration
# Phase 6: Direct Preference Optimization

# Models
models:
  policy_checkpoint: "out-sft/ckpt.pt"    # Initial policy (SFT model)
  # Note: No reward model needed for DPO

# DPO Hyperparameters
dpo:
  beta: 0.1               # Temperature parameter for implicit reward
  label_smoothing: 0.0    # Optional label smoothing

# Data
data:
  train_path: "data/preferences/train.json"
  val_path: "data/preferences/val.json"
  max_length: 512

# Training
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-7
  weight_decay: 0.01
  warmup_steps: 50
  max_steps: 2000
  eval_interval: 200
  save_interval: 500

# Output
output:
  dir: "out-dpo"
  log_interval: 10
