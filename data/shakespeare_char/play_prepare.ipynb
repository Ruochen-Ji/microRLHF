{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "194e73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ruochen/projects/nanoGPT/data/shakespeare_char\n",
      "length of dataset in characters:  1115394\n",
      "Vocab size is  65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "base_dir = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "print(base_dir)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size is \", vocab_size)\n",
    "print(''.join(chars))\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data[:10])\n",
    "# length of dataset in characters:  1115394\n",
    "# all the unique characters:\n",
    "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
    "# vocab size: 65\n",
    "# train has 1003854 tokens\n",
    "# val has 111540 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322511ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data[:10])\n",
    "print(val_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2e8bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what the max context window that the model can see.\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a907e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# [1,2,3,4,5,6,7,8,9,10...]\n",
    "# [0:8]\n",
    "x = train_data[:block_size]\n",
    "# [1:9]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is going to be a four random integers\n",
    "    # for example, [ 76049, 234249, 934904, 560986]\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,) )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        With nn.Embedding:\n",
    "        self.token_embedding_table = nn.Embedding(65, 65)\n",
    "        PyTorch automatically knows this is a learnable parameter!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx)  # idx is [4, 8]\n",
    "        ```\n",
    "\n",
    "        What PyTorch does:\n",
    "        1. Takes each token ID in your `[4, 8]` input\n",
    "        2. Looks up that token's row in the embedding table\n",
    "        3. Returns all 65 scores from that row\n",
    "\n",
    "        **Concrete example**: Let's trace position [0, 0] (first sequence, first token):\n",
    "        - `xb[0, 0]` might be token `23`\n",
    "        - Look up row 23 in the embedding table\n",
    "        - Get back 65 numbers (the scores for what comes after token 23)\n",
    "        - Store this at `logits[0, 0, :]` which has size 65\n",
    "\n",
    "        This happens **independently** for all 4×8 = 32 positions!\n",
    "\n",
    "        ### Step 4: The Output Shape\n",
    "        ```\n",
    "        Input:  [4, 8]       → 4 sequences × 8 tokens each\n",
    "                            ↓ lookup each token\n",
    "        Output: [4, 8, 65]   → 4 sequences × 8 positions × 65 scores for next token\n",
    "        ```\n",
    "\n",
    "        ## Intuitive Visualization\n",
    "\n",
    "        Imagine you have the sentence \"Hello my name is\":\n",
    "        ```\n",
    "        Position 0: \"Hello\"  → Get 65 scores for what comes after \"Hello\"\n",
    "        Position 1: \"my\"     → Get 65 scores for what comes after \"my\"  \n",
    "        Position 2: \"name\"   → Get 65 scores for what comes after \"name\"\n",
    "        Position 3: \"is\"     → Get 65 scores for what comes after \"is\"\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        # Use negative likelihood loss. \n",
    "        loss = F.cross_entropy()\n",
    "\n",
    "        return logits\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f3357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
