{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "194e73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ruochen/projects/nanoGPT/data/shakespeare_char\n",
      "length of dataset in characters:  1115394\n",
      "Vocab size is  65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "base_dir = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "print(base_dir)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size is \", vocab_size)\n",
    "print(''.join(chars))\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) \n",
    "print(data[:10])\n",
    "# length of dataset in characters:  1115394\n",
    "# all the unique characters:\n",
    "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
    "# vocab size: 65\n",
    "# train has 1003854 tokens\n",
    "# val has 111540 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "322511ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data[:10])\n",
    "print(val_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2e8bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what the max context window that the model can see.\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a907e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "# [1,2,3,4,5,6,7,8,9,10...]\n",
    "# [0:8]\n",
    "x = train_data[:block_size]\n",
    "# [1:9]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eea1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is going to be a four random integers\n",
    "    # for example, [ 76049, 234249, 934904, 560986]\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,) )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d1e186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        With nn.Embedding:\n",
    "        self.token_embedding_table = nn.Embedding(65, 65)\n",
    "        PyTorch automatically knows this is a learnable parameter!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        logits = self.token_embedding_table(idx)  # idx is [4, 8]\n",
    "        ```\n",
    "\n",
    "        What PyTorch does:\n",
    "        1. Takes each token ID in your `[4, 8]` input\n",
    "        2. Looks up that token's row in the embedding table\n",
    "        3. Returns all 65 scores from that row\n",
    "\n",
    "        **Concrete example**: Let's trace position [0, 0] (first sequence, first token):\n",
    "        - `xb[0, 0]` might be token `23`\n",
    "        - Look up row 23 in the embedding table\n",
    "        - Get back 65 numbers (the scores for what comes after token 23)\n",
    "        - Store this at `logits[0, 0, :]` which has size 65\n",
    "\n",
    "        This happens **independently** for all 4×8 = 32 positions!\n",
    "\n",
    "        ### Step 4: The Output Shape\n",
    "        ```\n",
    "        Input:  [4, 8]       → 4 sequences × 8 tokens each\n",
    "                            ↓ lookup each token\n",
    "        Output: [4, 8, 65]   → 4 sequences × 8 positions × 65 scores for next token\n",
    "        ```\n",
    "\n",
    "        ## Intuitive Visualization\n",
    "\n",
    "        Imagine you have the sentence \"Hello my name is\":\n",
    "        ```\n",
    "        Position 0: \"Hello\"  → Get 65 scores for what comes after \"Hello\"\n",
    "        Position 1: \"my\"     → Get 65 scores for what comes after \"my\"  \n",
    "        Position 2: \"name\"   → Get 65 scores for what comes after \"name\"\n",
    "        Position 3: \"is\"     → Get 65 scores for what comes after \"is\"\n",
    "        \"\"\"\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # [batch_size, block_size, vocab_size]\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # The corss entropy loss expects B * T, C \n",
    "            logits = logits.view(B * T, C)\n",
    "            # The reason we don't need C for target is target is the source of truth that as the \n",
    "            # exact next character's value. \n",
    "            targets = targets.view(B * T)\n",
    "            # Use negative likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step, shape should be (Batch, Channel)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            # dim tells which direction to calculate the probability\n",
    "            # we want the softmax to be calculated in the last dimension since it's channel. \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss= m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(type(loss))\n",
    "\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad480a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7182841300964355\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Getting the gradient of all the parameters in the model.\n",
    "    loss.backward()\n",
    "    # Use the gradient to update the parameters. \n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4150688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "knLI!UEHwcObxfNCha!qKt-ocPTFjyXrF\n",
      "W:..ZKAL.AHA.P!HAXNw,,$zCJ-!or'yxabLWGfkKowCXNe:g;gXEG'uVJMJ$\n",
      "&AkfNfq-GXlay!B?!\n",
      "SP JsBo.d,jIgEQzkq$YCZTOiqErphq?$zrzGJl3'IoiKIFuJuw\n",
      "CM\n",
      "&C-3\n",
      ".yff;DRj:Td,&uDK$Wj;Y -w?XXXEG:iPDtR'd,t\n",
      "-EHA3fxRObotE-wGRJiPmG'wyaIsr&NCj;IgLIas:C ?OgYQcM,jNCO.\n",
      "XXgwzLlaPm$VlgY.rbXSP fyN.N&\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7590f",
   "metadata": {},
   "source": [
    "The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb495524",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "# We want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "# b form 0 to B-1\n",
    "for b in range(B):\n",
    "    # t form 0 to T-1\n",
    "    for t in range(T):\n",
    "        # xprev is the previous tokens up to the current token. \n",
    "        # We are averaging the columns vertically because we want the average \n",
    "        # of all the previous tokens.\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcac9762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape\n",
    "torch.tril(torch.ones(4, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "942dd3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "Max difference: 3.236345946788788e-08\n"
     ]
    }
   ],
   "source": [
    "# print out these to see the averages\n",
    "x[0]\n",
    "xbow[0]\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "# Summing the second dimension, we get the sum of each row. \n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) -> (B, T, C) pytorch automatically broadcasts the 1 to B\n",
    "diff = (xbow - xbow2).abs().max()\n",
    "print(f\"Max difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "559eb374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "# Token from the past cannot communicate. By setting them to -inf, we will not aggregate anything from those tokens from those \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff79fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# T x T tensor\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print('a =', a)\n",
    "print('b =', b)\n",
    "print('c =', c)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda19d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embed = 32\n",
    "device = \"cuda\"\n",
    "\n",
    "class BigramLanguageModelImproved(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # [B, T, C (n_embed)]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # [T, C (n_embed)]\n",
    "        x = token_emb + pos_emb  # [B, T, C (n_embed)]\n",
    "        logits = self.lm_head(x)  # [B, T, vocab_size]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # The corss entropy loss expects B * T, C \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # Use negative likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step, shape should be (Batch, Channel)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            # dim tells which direction to calculate the probability\n",
    "            # we want the softmax to be calculated in the last dimension since it's channel. \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bab21089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now code out the attention mechanism. \n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, sequence length, channel\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single Head perform self-attention.\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "# out = wei @ x\n",
    "\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97145123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc7b2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"One head of self-attention\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "        # Self attntion\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841d6d1",
   "metadata": {},
   "source": [
    "### The new BigramLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45031599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embed = 32\n",
    "device = \"cuda\"\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32   \n",
    "\n",
    "class BigramLanguageModelWithSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # self-attention head\n",
    "        self.sa_head = Head(head_size=n_embed)\n",
    "        # multi-head self-attention\n",
    "        self.mha = MultiHeadAttention(num_heads=4, head_size=n_embed // 4)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # [B, T, C (n_embed)]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # [T, C (n_embed)]\n",
    "        x = token_emb + pos_emb  # [B, T, C (n_embed)]\n",
    "        # Single head self-attention    \n",
    "        # x = self.sa_head(x) # (B, T, n_embed/head_size)\n",
    "        # Multi-head self-attention\n",
    "        x = self.mha(x) # (B, T, n_embed)\n",
    "        logits = self.lm_head(x)  # [B, T, vocab_size]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # The corss entropy loss expects B * T, C \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # Use negative likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step, shape should be (Batch, Channel)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            # dim tells which direction to calculate the probability\n",
    "            # we want the softmax to be calculated in the last dimension since it's channel. \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d17bd",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceec7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.251319169998169\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModelWithSelfAttention()\n",
    "m = m.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is going to be a four random integers\n",
    "    # for example, [ 76049, 234249, 934904, 560986]\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,) )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "for steps in range(10):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Getting the gradient of all the parameters in the model.\n",
    "    loss.backward()\n",
    "    # Use the gradient to update the parameters. \n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9aa0469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whent ik bridcowf,\n",
      "Tkis soret mad selabube toe.\n",
      "Sagrtand thalied\n",
      "hy ard that usqurthe.\n",
      "War dilth ate\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68999284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem with the current model is we went way too fast to calculate the logits.\n",
    "# The tokens look at each other but didn't really have time to think on what they have found from other toknes.\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed-forward network\"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0e6722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embed = 32\n",
    "device = \"cuda\"\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32   \n",
    "\n",
    "class BigramLanguageModelWithSelfAttentionWithFeedforward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        # self-attention head\n",
    "        self.sa_head = Head(head_size=n_embed)\n",
    "        # multi-head self-attention\n",
    "        self.mha = MultiHeadAttention(num_heads=4, head_size=n_embed // 4) # Output dimension: (B, T, n_embed)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # [B, T, C (n_embed)]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # [T, C (n_embed)]\n",
    "        x = token_emb + pos_emb  # [B, T, C (n_embed)]\n",
    "        # Single head self-attention    \n",
    "        # x = self.sa_head(x) # (B, T, n_embed/head_size)\n",
    "        # Multi-head self-attention\n",
    "        x = self.mha(x) # (B, T, c(n_embed))\n",
    "        x = self.ffwd(x) # (B, T, c(n_embed))\n",
    "        logits = self.lm_head(x)  # [B, T, vocab_size]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # The corss entropy loss expects B * T, C \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # Use negative likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step, shape should be (Batch, Channel)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            # dim tells which direction to calculate the probability\n",
    "            # we want the softmax to be calculated in the last dimension since it's channel. \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37f032e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The residual connection is important for the model to learn. \n",
    "        It allows the model to learn the identity function. \n",
    "        Without it, the model will have a hard time learning. \n",
    "        Here is the code without residual connection:\n",
    "        # x = self.sa(x)\n",
    "        # x = self.ffwd(x)\n",
    "        \"\"\"\n",
    "        # With residual connection\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"A new multi-head attention layer that uses projection\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed-forward network\"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, n_embed)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f49b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embed = 32\n",
    "device = \"cuda\"\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32   \n",
    "\n",
    "class BigramLanguageModelAttentionWithoutLayerNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(Block(n_embed, n_heads=4), Block(n_embed, n_heads=4), Block(n_embed, n_heads=4))\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # [B, T, C (n_embed)]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # [T, C (n_embed)]\n",
    "        x = token_emb + pos_emb  # [B, T, C (n_embed)]\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)  # [B, T, vocab_size]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # The corss entropy loss expects B * T, C \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # Use negative likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step, shape should be (Batch, Channel)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            # dim tells which direction to calculate the probability\n",
    "            # we want the softmax to be calculated in the last dimension since it's channel. \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41e9419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now introduce layer norm to the model.\n",
    "class BlockWithLayerNorm(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation with layer norm\"\"\"\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7565c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embed = 32\n",
    "device = \"cuda\"\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32   \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple feed-forward network\"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class BigramLanguageModelAttentionFinal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(BlockWithLayerNorm(n_embed, n_heads=4), BlockWithLayerNorm(n_embed, n_heads=4), BlockWithLayerNorm(n_embed, n_heads=4), nn.LayerNorm(n_embed))\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_emb = self.token_embedding_table(idx)  # [B, T, C (n_embed)]\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # [T, C (n_embed)]\n",
    "        x = token_emb + pos_emb  # [B, T, C (n_embed)]\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)  # [B, T, vocab_size]\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # The corss entropy loss expects B * T, C \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            # Use negative likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step, shape should be (Batch, Channel)\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            # dim tells which direction to calculate the probability\n",
    "            # we want the softmax to be calculated in the last dimension since it's channel. \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97cf8f",
   "metadata": {},
   "source": [
    "### Let's train the model again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84728867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9977909326553345\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModelAttentionFinal()\n",
    "m = m.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is going to be a four random integers\n",
    "    # for example, [ 76049, 234249, 934904, 560986]\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,) )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "for steps in range(max_iters):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Getting the gradient of all the parameters in the model.\n",
    "    loss.backward()\n",
    "    # Use the gradient to update the parameters. \n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3837b2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "JUCHARD:\n",
      "I wcowill to lawer K\n",
      "bady;\n",
      "Thou but: and O-dam meall and bard thy pusquet to bardetlaccan,\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbdf35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
